{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d46607",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data (run once)\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "def simulate_data(num_records=1000):\n",
    "    \"\"\"\n",
    "    Simulates a customer feedback dataset with a mix of positive, negative, and neutral reviews.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Simple list of words/phrases for simulation\n",
    "    positive_terms = ['great service', 'excellent product', 'highly recommend', 'fast delivery', 'love it', 'very helpful', 'amazing quality']\n",
    "    negative_terms = ['slow response', 'broken item', 'terrible experience', 'unacceptable quality', 'never again', 'frustrated with delay', 'poor support']\n",
    "    neutral_terms = ['no change noticed', 'it works as expected', 'standard delivery time', 'average performance', 'received the package']\n",
    "    \n",
    "    feedbacks = []\n",
    "    sentiments = []\n",
    "\n",
    "    for i in range(num_records):\n",
    "        # Determine sentiment and associated phrase\n",
    "        if i < num_records * 0.4: # 40% Positive\n",
    "            sentiment = 'Positive'\n",
    "            term = np.random.choice(positive_terms)\n",
    "        elif i < num_records * 0.7: # 30% Negative\n",
    "            sentiment = 'Negative'\n",
    "            term = np.random.choice(negative_terms)\n",
    "        else: # 30% Neutral\n",
    "            sentiment = 'Neutral'\n",
    "            term = np.random.choice(neutral_terms)\n",
    "\n",
    "        # Add noise and complexity\n",
    "        noise = np.random.choice(['!', '??', '... ', ' ', '@user ', '#tag '], p=[0.1, 0.1, 0.2, 0.5, 0.05, 0.05])\n",
    "        text = f\"The {term}{noise}.\"\n",
    "        \n",
    "        # Introduce a few duplicates\n",
    "        if i % 100 == 0 and i > 0:\n",
    "            feedbacks.append(feedbacks[-1]) # Add exact duplicate\n",
    "            sentiments.append(sentiments[-1])\n",
    "            \n",
    "        feedbacks.append(text.lower().strip())\n",
    "        sentiments.append(sentiment)\n",
    "\n",
    "    df = pd.DataFrame({'feedback_id': range(len(feedbacks)), 'feedback_text': feedbacks, 'sentiment': sentiments})\n",
    "    # Add some missing values for robust cleaning\n",
    "    df.loc[df.sample(frac=0.01).index, 'feedback_text'] = np.nan\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_and_preprocess(df):\n",
    "    \"\"\"\n",
    "    Performs data cleaning and preprocessing steps.\n",
    "    \"\"\"\n",
    "    print(f\"Initial shape: {df.shape}\")\n",
    "    \n",
    "    # 1. Handle missing data (Dropping rows with missing feedback text)\n",
    "    df.dropna(subset=['feedback_text'], inplace=True)\n",
    "    print(f\"Shape after handling missing values: {df.shape}\")\n",
    "    \n",
    "    # 2. Remove duplicates\n",
    "    df.drop_duplicates(subset=['feedback_text'], keep='first', inplace=True)\n",
    "    print(f\"Shape after removing duplicates: {df.shape}\")\n",
    "    \n",
    "    # 3. Clean Text (Remove special characters, tags, links)\n",
    "    def clean_text(text):\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        # Remove mentions and hashtags\n",
    "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "        # Remove special characters and numbers (keeping only letters and spaces)\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        # Lowercase and strip is already done in simulation, but good practice here\n",
    "        return text.lower().strip()\n",
    "\n",
    "    df['cleaned_text'] = df['feedback_text'].apply(clean_text)\n",
    "\n",
    "    # 4. Tokenization, Stopword Removal, and Lemmatization\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def preprocess_text(text):\n",
    "        # Tokenization (split by space after cleaning)\n",
    "        tokens = text.split()\n",
    "        # Stopword removal\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        # Lemmatization\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    # Note: For BERT/Transformer models (Part 2), you often skip stopword removal/lemmatization\n",
    "    # and rely on the model's tokenizer. We'll keep this step here for traditional models,\n",
    "    # but for a BERT-only approach, we'd typically use 'cleaned_text' directly.\n",
    "    df['processed_text'] = df['cleaned_text'].apply(preprocess_text)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == '__main__':\n",
    "    # Step 1: Simulate data\n",
    "    feedback_data = simulate_data(num_records=1100)\n",
    "    print(\"--- Initial Data Sample ---\")\n",
    "    print(feedback_data.head())\n",
    "    \n",
    "    # Step 2: Clean and preprocess data\n",
    "    cleaned_data = clean_and_preprocess(feedback_data)\n",
    "    \n",
    "    print(\"\\n--- Cleaned and Processed Data Sample ---\")\n",
    "    print(cleaned_data[['feedback_text', 'cleaned_text', 'processed_text', 'sentiment']].sample(5))\n",
    "\n",
    "    # Deliverable: Save the cleaned dataset\n",
    "    # You would typically save the 'cleaned_text' column for BERT training\n",
    "    # or 'processed_text' for LSTM/traditional models.\n",
    "    cleaned_data.to_csv('cleaned_customer_feedback.csv', index=False)\n",
    "    print(\"\\nCleaned dataset saved to 'cleaned_customer_feedback.csv'.\")\n",
    "\n",
    "    # The 'cleaned_customer_feedback.csv' file is your deliverable.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
